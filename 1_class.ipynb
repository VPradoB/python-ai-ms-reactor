{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e037557-4f03-4ee6-ab23-57c5c29b33a8",
   "metadata": {},
   "source": [
    "# Como funcionan los LLM\n",
    "Los LLM (Modelos de Lenguaje de Gran Tama√±o) son una clase de modelos de inteligencia artificial dise√±ados para comprender y generar texto en lenguaje natural. Funcionan gracias a una arquitectura llamada **Transformers**, introducida por Google en 2017.\n",
    "\n",
    "## 1. **Entrenamiento con grandes cantidades de texto**\n",
    "Un LLM es entrenado con billones de palabras extra√≠das de libros, art√≠culos, sitios web y otros documentos. Durante este proceso, el modelo aprende a **predecir la siguiente palabra** dado un contexto. Por ejemplo, si se le da \"El cielo est√°\", el modelo aprende que lo m√°s probable es que la siguiente palabra sea \"azul\".\n",
    "\n",
    "## 2. **Tokens, no palabras**\n",
    "En lugar de procesar palabras enteras, los LLM trabajan con *tokens*, que pueden ser palabras, fragmentos de palabras o incluso caracteres. Esto les permite manejar vocabularios m√°s amplios y comprender mejor estructuras gramaticales complejas.\n",
    "\n",
    "## 3. **Representaci√≥n num√©rica**\n",
    "Cada token es transformado en un vector de n√∫meros mediante una capa de *embedding*. Estos vectores permiten representar relaciones sem√°nticas: por ejemplo, los vectores de \"rey\" y \"reina\" estar√°n cerca en el espacio matem√°tico.\n",
    "\n",
    "## 4. **Arquitectura Transformer**\n",
    "La clave del √©xito de los LLM est√° en los *Transformers*. Su componente principal es el **self-attention**, que permite que el modelo considere todas las palabras del contexto al mismo tiempo, asignando distintos pesos a cada una dependiendo de su relevancia. Esto lo hace extremadamente eficaz para comprender el significado de una oraci√≥n en su totalidad.\n",
    "## 5. **Generalizaci√≥n y generaci√≥n**\n",
    "Una vez entrenado, el modelo puede generalizar a textos nunca vistos. Por ejemplo, puede generar respuestas, traducir idiomas, resumir textos o responder preguntas. Todo esto se logra simplemente prediciendo token por token cu√°l es la salida m√°s probable dada la entrada.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A(Prompt) --> B(Tokenizaci√≥n)\n",
    "    B --> C1(Embeddings)\n",
    "    C1 --> C2(Atenci√≥n)\n",
    "    C2 --> C3(Transformers)\n",
    "    C3 --> D[Distribuci√≥n de Probabilidad]\n",
    "    D --> E(Selecci√≥n / Decodificaci√≥n)\n",
    "    E --> F(Texto Generado)\n",
    "\n",
    "    subgraph Magia_del_Modelo [Magia del Modelo]\n",
    "        direction TB\n",
    "        C1\n",
    "        C2\n",
    "        C3\n",
    "    end\n",
    "\n",
    "    style A fill:#f9f,stroke:#333,stroke-width:2px\n",
    "    style F fill:#ccf,stroke:#333,stroke-width:2px\n",
    "    style Magia_del_Modelo fill:#ff9,stroke:#333,stroke-width:2px,color:#000\n",
    "    style D fill:#fec,stroke:#333,stroke-width:1px\n",
    "    style E fill:#bfb,stroke:#333,stroke-width:1px\n",
    "```\n",
    "\n",
    "# Tipos de Modelos LLM, SLM y Modelos Locales\n",
    "\n",
    "## üß† 1. **LLM (Large Language Models) en la nube**\n",
    "Estos modelos son alojados por grandes empresas tecnol√≥gicas y requieren conexi√≥n a internet. Son potentes y ofrecen capacidades de razonamiento, generaci√≥n y comprensi√≥n avanzadas.\n",
    "\n",
    "| Modelo        | Proveedor        | Observaciones |\n",
    "|---------------|------------------|----------------|\n",
    "| GPT-4 / GPT-3.5 | OpenAI (Azure o API directa) | GPT-4 es multimodal y m√°s preciso, pero m√°s costoso |\n",
    "| Claude 3       | Anthropic        | Enfocado en seguridad, excelente en instrucciones complejas |\n",
    "| Gemini 1.5     | Google (ex Bard) | Fuerte en tareas razonadas, multimodal |\n",
    "| Mistral API    | Mistral.ai       | Modelos como Mistral 7B y Mixtral (MoE), ligeros y r√°pidos |\n",
    "| Command R+     | Cohere           | Optimizado para RAG (Retrieval-Augmented Generation) |\n",
    "| LLaMA 2/3      | Meta (a trav√©s de servicios como Together.ai) | Open-weight pero generalmente servidos v√≠a API |\n",
    "| Jurassic       | AI21 Labs        | Compatible con tareas tipo GPT |\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ 2. **SLM (Small Language Models)**\n",
    "Modelos m√°s peque√±os, ideales para despliegue en entornos con recursos limitados o tareas espec√≠ficas.\n",
    "\n",
    "| Modelo        | Tama√±o (Par√°metros) | Observaciones |\n",
    "|---------------|---------------------|----------------|\n",
    "| TinyLLaMA     | 1.1B                | Entrenado en bajo costo, ideal para m√≥viles o edge |\n",
    "| Phi-2         | 2.7B                | Microsoft. Excelente rendimiento para su tama√±o |\n",
    "| Gemma         | 2B / 7B             | Google. Enfocado en eficiencia y open-source |\n",
    "| Mistral 7B    | 7B                  | Alta calidad y velocidad, muy vers√°til |\n",
    "| Falcon 1B/7B  | 1B / 7B             | Modelos eficientes lanzados por TII (EAU) |\n",
    "| Orca 2        | 7B+                 | Fine-tuned para habilidades de razonamiento |\n",
    "\n",
    "---\n",
    "\n",
    "## üè° 3. **Modelos Locales (para ejecutar en tu m√°quina)**\n",
    "Perfectos para entornos sin conexi√≥n o donde necesitas control total. Se usan con herramientas como `llama.cpp`, `Ollama`, `LM Studio`, `Text Generation Web UI`, etc.\n",
    "\n",
    "| Modelo        | Plataforma de soporte | Requisitos |\n",
    "|---------------|------------------------|-------------|\n",
    "| LLaMA 2 / 3    | `llama.cpp`, Ollama    | GPU recomendada para 13B+ |\n",
    "| Mistral 7B     | Ollama, `gguf`, LM Studio | Muy popular y balanceado |\n",
    "| Gemma          | Ollama, Hugging Face  | Puede correr sin GPU en CPUs modernas |\n",
    "| TinyLLaMA      | `llama.cpp`, Web UI   | Extremadamente liviano |\n",
    "| OpenHermes     | Web UI, Ollama        | Variante fine-tuned con instrucciones |\n",
    "| Neural Chat    | Intel/OpenVINO        | Optimizados para CPUs Intel |\n",
    "| Phi-2          | `transformers`, ONNX  | Muy r√°pido con aceleraci√≥n CPU |\n",
    "| StarCoder      | Ollama, Web UI        | Ideal para tareas de codificaci√≥n |\n",
    "\n",
    "---\n",
    "\n",
    "**Nota:** Para correr modelos localmente de forma eficiente, se recomienda usar versiones en formato `GGUF` con `llama.cpp` o `Ollama`, que permite cargar modelos optimizados para tu CPU o GPU.\n",
    "\n",
    "\n",
    "\n",
    "# üêç Librer√≠as en Python para usar LLMs\n",
    "Aqu√≠ est√°n las principales librer√≠as para trabajar con modelos de lenguaje desde Python, ya sea conect√°ndote a servicios externos o ejecut√°ndolos localmente:\n",
    "## üîå 1. **Uso de LLMs v√≠a API (OpenAI, Anthropic, Cohere, etc.)**\n",
    "\n",
    "| Librer√≠a        | Descripci√≥n |\n",
    "|-----------------|-------------|\n",
    "| `openai`        | Cliente oficial para usar GPT-3.5 / GPT-4 con la API de OpenAI. |\n",
    "| `anthropic`     | Cliente oficial para usar los modelos Claude (Claude 1, 2, 3). |\n",
    "| `google.generativeai` | Cliente de Gemini (antes Bard). |\n",
    "| `cohere`        | Cliente para los modelos Command-R, muy √∫til en RAG. |\n",
    "| `transformers` (con `text-generation-inference`) | Puede conectarse a servidores remotos de modelos tipo Hugging Face. |\n",
    "| `httpx` / `requests` | Para hacer llamadas directas a APIs REST de cualquier proveedor. |\n",
    "\n",
    "---\n",
    "\n",
    "## üß† 2. **Uso de modelos open-source y locales**\n",
    "\n",
    "| Librer√≠a            | Descripci√≥n |\n",
    "|---------------------|-------------|\n",
    "| `transformers` (Hugging Face) | La librer√≠a m√°s completa para cargar y usar modelos como GPT-2, LLaMA, Falcon, etc. |\n",
    "| `accelerate`        | Optimiza la carga y ejecuci√≥n de modelos grandes en CPU o GPU. |\n",
    "| `llama-cpp-python`  | Python bindings para `llama.cpp`, permite correr modelos como LLaMA, Mistral en CPU o GPU. |\n",
    "| `ollama`            | Usa modelos locales f√°cilmente con una API local (`http://localhost:11434/api`). |\n",
    "| `ctransformers`     | Ejecuta modelos ligeros (como GGML/GGUF) sin necesidad de GPU. |\n",
    "| `vllm`              | Ejecuta LLMs con alta eficiencia en GPU, ideal para servir modelos grandes. |\n",
    "| `langchain`         | Framework para orquestar LLMs, hacer chains, memory, agentes, etc. |\n",
    "| `llama-index` (ex GPT Index) | Facilita el uso de LLMs para RAG (b√∫squeda aumentada por recuperaci√≥n). |\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è 3. **Complementos √∫tiles**\n",
    "\n",
    "| Librer√≠a     | Uso |\n",
    "|--------------|-----|\n",
    "| `gradio`     | Crear interfaces web para probar modelos f√°cilmente. |\n",
    "| `streamlit`  | Ideal para dashboards interactivos con LLMs. |\n",
    "| `pydantic`   | Validaci√≥n de datos para entradas/salidas con LLMs. |\n",
    "| `dotenv`     | Manejo de claves API de forma segura. |\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b62b2c9-a278-43ca-a3e9-b1d3f2e29c30",
   "metadata": {},
   "source": [
    "# üìå Ejemplos pr√°cticos\n",
    "\n",
    "A continuaci√≥n puedes consultar ejemplos pr√°cticos de c√≥mo interactuar con LLMs desde Python utilizando dos APIs diferentes:\n",
    "\n",
    "- ü§ñ [Ejemplo usando la API de Gemini](./code_example/01_hello_world_gemini.ipynb)\n",
    "- üß† [Ejemplo usando la API de OpenAI](./code_example/01_hello_world_openai.ipynb)\n",
    "\n",
    "Tambi√©n puedes explorar c√≥mo manejar respuestas **en tiempo real (streaming)**:\n",
    "\n",
    "- üì° [Streaming con OpenAI (respuesta progresiva)](./code_example/02_chat_stream_openai.ipynb)\n",
    "- üö´ [Intento de streaming con Gemini (no soportado)](./code_example/02_chat_stream_gemini.ipynb)\n",
    "\n",
    "Todos los ejemplos siguen la misma l√≥gica: configurar el modelo, enviar una entrada con una instrucci√≥n de sistema (prompt), y generar una respuesta. La diferencia est√° en la librer√≠a usada y el proveedor del modelo\n",
    "\n",
    "---\n",
    "# üõ†Ô∏è Maneras de mejorar el output de un LLM\n",
    "\n",
    "## üéØ Prompt Engineering\n",
    "## üé≠ Roles en la API de OpenAI\n",
    "\n",
    "Cuando trabajas con `chat.completions` en la API de OpenAI (por ejemplo con modelos como GPT-3.5 o GPT-4), se utiliza una conversaci√≥n estructurada con distintos **roles**. Cada rol tiene un prop√≥sito espec√≠fico en el flujo del chat.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∞ Roles disponibles\n",
    "\n",
    "| Rol         | Descripci√≥n                                                                 |\n",
    "|-------------|-----------------------------------------------------------------------------|\n",
    "| `system`    | Define el comportamiento, tono o personalidad del modelo.                  |\n",
    "| `user`      | Representa lo que el usuario est√° preguntando o solicitando.               |\n",
    "| `assistant` | Representa la respuesta previa del modelo, √∫til para mantener el contexto. |\n",
    "\n",
    "---\n",
    "\n",
    "### üß© ¬øPara qu√© sirve cada uno?\n",
    "\n",
    "#### üß† `system`\n",
    "Este mensaje sirve para establecer el **contexto inicial**. Puedes definir el rol del modelo, su estilo de respuesta, conocimientos preferidos o su actitud.\n",
    "\n",
    "```json\n",
    "{ \"role\": \"system\", \"content\": \"Eres un nutricionista que da consejos para una dieta saludable sin mencionar carne roja.\" }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üåü Roles en la API de Gemini\n",
    "\n",
    "Al trabajar con modelos de Gemini (como `gemini-1.5-pro`) a trav√©s de la librer√≠a `google.generativeai`, la estructura de los mensajes tambi√©n incluye roles, aunque con algunas diferencias respecto a OpenAI.\n",
    "\n",
    "---\n",
    "\n",
    "### üî∞ Roles disponibles\n",
    "\n",
    "| Rol   | Descripci√≥n                                                           |\n",
    "|--------|-----------------------------------------------------------------------|\n",
    "| `user` | Mensajes enviados por el usuario, los que contienen las preguntas.    |\n",
    "| `model` (opcional en input) | Mensajes generados por el modelo, √∫tiles si deseas mantener contexto.   |\n",
    "| `system` | No es parte de los mensajes, sino una **instrucci√≥n separada** llamada `system_instruction`. |\n",
    "\n",
    "---\n",
    "\n",
    "### üß© ¬øPara qu√© sirve cada uno?\n",
    "\n",
    "#### üßë‚Äçüíª `user`\n",
    "Representa el texto de entrada o prompt enviado al modelo. Por ejemplo:\n",
    "\n",
    "```python\n",
    "contents = [{\"role\": \"user\", \"parts\": [{\"text\": \"¬øCu√°l es la ra√≠z cuadrada de 144?\"}]}]\n",
    "```\n",
    "\n",
    "## üß™ Few-shot Examples\n",
    "Demostrarle al modelo c√≥mo deber√≠a verse la salida proporcionando uno o m√°s ejemplos dentro del prompt.\n",
    "\n",
    "---\n",
    "\n",
    "### ü§ñ En OpenAI\n",
    "\n",
    "En la API de OpenAI, puedes usar el rol `assistant` para mostrar ejemplos previos junto con el usuario (`user`). Esto le ayuda al modelo a aprender el formato deseado.\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"Eres un traductor de espa√±ol a ingl√©s\"},\n",
    "    {\"role\": \"user\", \"content\": \"¬øC√≥mo se dice 'hola' en ingl√©s?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello\"},\n",
    "    {\"role\": \"user\", \"content\": \"¬øC√≥mo se dice 'gracias' en ingl√©s?\"}\n",
    "]\n",
    "```\n",
    "### ü§ñ En Gemini\n",
    "Gemini no tiene un rol assistant, pero puedes lograr el mismo efecto usando el rol model para mostrar c√≥mo el modelo deber√≠a responder, y construir una conversaci√≥n similar.\n",
    "```python\n",
    "contents = [\n",
    "    {\"role\": \"user\", \"parts\": [{\"text\": \"¬øC√≥mo se dice 'hola' en ingl√©s?\"}]},\n",
    "    {\"role\": \"model\", \"parts\": [{\"text\": \"Hello\"}]},\n",
    "    {\"role\": \"user\", \"parts\": [{\"text\": \"¬øC√≥mo se dice 'gracias' en ingl√©s?\"}]}\n",
    "]\n",
    "```\n",
    "## üîó Llamadas Encadenadas\n",
    "Implica dividir una tarea en m√∫ltiples pasos y hacer que el modelo los resuelva en cadena. Esto permite que el LLM reflexione, piense en voz alta o procese por partes.\n",
    "\n",
    "üìå Ejemplo de flujo:\n",
    "1. ‚ÄúAnaliza el problema‚Äù\n",
    "2. ‚ÄúIdentifica pasos a seguir‚Äù\n",
    "3. ‚ÄúResuelve paso a paso‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "### üìö Otras t√©cnicas (ser√°n cubiertas en clases futuras)\n",
    "\n",
    "- üß† **Recuperaci√≥n-Aumentada Generaci√≥n (RAG)**: Proveer contexto al modelo en tiempo real.\n",
    "- üõ†Ô∏è **Llamada a funciones y salidas estructuradas**\n",
    "- üß¨ **Fine-tuning**: Ense√±ar al modelo con nueva informaci√≥n alterando sus pesos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39beda7a-1553-4837-9f42-ec50b3190430",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
